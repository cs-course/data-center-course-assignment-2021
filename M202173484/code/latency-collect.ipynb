{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验环境初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from boto3.session import Session\n",
    "import botocore\n",
    "from tqdm.notebook import tqdm\n",
    "from ratelimiter import RateLimiter\n",
    "\n",
    "# 准备密钥\n",
    "aws_access_key_id = 'PG3XVLGJ0PI4EWF0SWP3'\n",
    "aws_secret_access_key = 'QCyuKS2FIq40RpTiRushOh5NblkjmTGxt0hmDoav'\n",
    "\n",
    "# 本地S3服务地址\n",
    "local_s3 = 'http://master:8080'\n",
    "\n",
    "# 建立会话\n",
    "session = Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "# 连接到服务\n",
    "s3 = session.resource('s3', endpoint_url=local_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看所有bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket name:loadgen\n"
     ]
    }
   ],
   "source": [
    "for bucket in s3.buckets.all():\n",
    "    print('bucket name:%s' % bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建一个实验用 bucket (注意：\"bucket name\" 中不能有下划线)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket loadgen not found, creating new one...\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'loadgen'\n",
    "if s3.Bucket(bucket_name) not in s3.buckets.all():\n",
    "    print(f'bucket {bucket_name} not found, creating new one...')\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    print(f'skip bucket {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看此 bucket 下的所有 object (若之前实验没有正常结束，则不为空)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(bucket_name)\n",
    "for obj in bucket.objects.all():\n",
    "    print('obj name:%s' % obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备负载，可以按照几种不同请求到达率 (Inter-Arrival Time, IAT) 设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初始化本地数据文件\n",
    "local_file = \"_test_4K.bin\"\n",
    "test_bytes = [0xFF for i in range(1024*4)] # 填充至所需大小\n",
    "bytes_obj = io.BytesIO(bytearray(test_bytes))\n",
    "\n",
    "with open(local_file, \"wb\") as lf:\n",
    "    lf.write(bytearray(test_bytes))\n",
    "\n",
    "def direct_request(action, res, i, localfile):\n",
    "    start = time.time()\n",
    "    action(res, i, localfile)\n",
    "    end = time.time()\n",
    "    system_time = end - start\n",
    "    return system_time * 1000 # 换算为毫秒\n",
    "    \n",
    "def hedged_request(action, res, i, localfile):\n",
    "    start = time.time()\n",
    "    nThreads = 2\n",
    "    with ThreadPoolExecutor(max_workers=nThreads) as executor: # 通过 max_workers 设置并发线程数\n",
    "        futures = [executor.submit(arrival_rate_max, res, i, localfile + str(i)) for i in range(nThreads)]\n",
    "        for future in as_completed(futures):\n",
    "            # 只要第一个结果\n",
    "            end = time.time()\n",
    "            system_time = end - start\n",
    "            return system_time * 1000 # 换算为毫秒\n",
    "\n",
    "def tied_request(action, res, i, localfile):\n",
    "    start = time.time()\n",
    "    _95_percentile = 0.3 # 95%分位数时间，超过这个时间就丢弃这个请求，发送新的\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor: # 通过 max_workers 设置并发线程数\n",
    "        future = executor.submit(arrival_rate_max, res, i, localfile)\n",
    "        time.sleep(_95_percentile)\n",
    "        while not future.done():\n",
    "            future.cancel()\n",
    "            future = executor.submit(arrival_rate_max)\n",
    "            time.sleep(_95_percentile)\n",
    "        end = time.time()\n",
    "        system_time = end - start\n",
    "        return system_time * 1000 # 换算为毫秒\n",
    "\n",
    "    \n",
    "# 发起请求和计算系统停留时间\n",
    "def request_timing(s3res, i, localfile): # 使用独立 session.resource 以保证线程安全\n",
    "    obj_name = \"testObj%08d\"%(i,) # 所建对象名\n",
    "    # temp_file = '.tempfile'\n",
    "    s3res.Object(bucket_name, obj_name).download_file(localfile)\n",
    "\n",
    "# 按照请求到达率限制来执行和跟踪请求\n",
    "def arrival_rate_max(s3res, i, *args, **kwargs): # 不进行限速\n",
    "    return request_timing(s3res, i, *args, **kwargs)\n",
    "\n",
    "@RateLimiter(0.1, 2) # 0.1s 内不超过 2 个请求，下同……\n",
    "def arrival_rate_2(s3res, i, *args, **kwargs):\n",
    "    return request_timing(s3res, i, *args, **kwargs)\n",
    "\n",
    "@RateLimiter(0.1, 4)\n",
    "def arrival_rate_4(s3res, i, *args, **kwargs):\n",
    "    return request_timing(s3res, i, *args, **kwargs)\n",
    "\n",
    "@RateLimiter(0.1, 8)\n",
    "def arrival_rate_8(s3res, i, *args, **kwargs):\n",
    "    return request_timing(s3res, i, *args, **kwargs)\n",
    "\n",
    "@RateLimiter(0.1, 16)\n",
    "def arrival_rate_16(s3res, i, *args, **kwargs):\n",
    "    return request_timing(s3res, i, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照预设IAT发起请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb367a2a6084a90a967c8a2db28749e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accessing S3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latency = []\n",
    "failed_requests = []\n",
    "\n",
    "with tqdm(desc=\"Accessing S3\", total=100) as pbar:      # 进度条设置，合计执行 100 项上传任务 (见 submit 部分)，进度也设置为 100 步\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor: # 通过 max_workers 设置并发线程数\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                direct_request,\n",
    "                arrival_rate_max,\n",
    "                session.resource('s3', endpoint_url=local_s3),\n",
    "                i, bytes_obj) for i in range(100) # 为保证线程安全，应给每个任务申请一个新 resource\n",
    "            ]\n",
    "        for future in as_completed(futures):\n",
    "            if future.exception():\n",
    "                failed_requests.append(future)\n",
    "            else:\n",
    "                latency.append(future.result()) # 正确完成的请求，采集延迟\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    failed_requests[0].result()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清理实验环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 删除bucket下所有object\n",
    "    bucket.objects.all().delete()\n",
    "\n",
    "    # 删除bucket下某个object\n",
    "    # bucket.objects.filter(Prefix=obj_name).delete()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除本地测试文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录延迟到CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"latency.csv\", \"w+\") as tracefile:\n",
    "    tracefile.write(\"latency\\n\")\n",
    "    tracefile.writelines([str(l) + '\\n' for l in latency])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmark计划：\n",
    "1. 明确要测的指标（请求延迟）及目的（测量不同访问特征下的请求延迟，分析访问特征对请求延迟的同质影响（类正态分布、尾延迟）和异质影响（分布的具体参数））\n",
    "2. 明确影响待测指标的参数（文件大小、请求速率（每秒多少个请求）、访问策略）\n",
    "3. 设定想测试的参数，运行自动化测试并收集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size_kb = [4, 256, 1024, 4096]\n",
    "rate_limits = {\n",
    "    'max': arrival_rate_max\n",
    "    #'4by100ms': arrival_rate_4,\n",
    "    #'16by100ms': arrival_rate_16\n",
    "}\n",
    "policies = {\n",
    "    #'direct': direct_request,\n",
    "    #'hedged': hedged_request,\n",
    "    'tied': tied_request\n",
    "}\n",
    "client_count = [1, 16, 64]\n",
    "\n",
    "def bench(fileBuffer=None, rate_limit=None, policy=None, n_clients=1, nRequests=128):\n",
    "    assert(fileBuffer is not None)\n",
    "    assert(rate_limit is not None)\n",
    "    assert(policy is not None)\n",
    "    \n",
    "    _latency = []\n",
    "    _failed = []\n",
    "\n",
    "    with tqdm(desc=\"Accessing S3\", total=nRequests) as pbar:      # 进度条设置，合计执行128项上传任务\n",
    "        with ThreadPoolExecutor(max_workers=n_clients) as executor: # 通过 max_workers 设置并发线程数\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    policy,\n",
    "                    rate_limit,\n",
    "                    session.resource('s3', endpoint_url=local_s3),\n",
    "                    i, fileBuffer) for i in range(nRequests) # 为保证线程安全，应给每个任务申请一个新 resource\n",
    "                ]\n",
    "            for future in as_completed(futures):\n",
    "                if future.exception():\n",
    "                    _failed.append(future)\n",
    "                else:\n",
    "                    _latency.append(future.result()) # 正确完成的请求，采集延迟\n",
    "                pbar.update(1)\n",
    "        \n",
    "    return _latency, _failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing file size 4K ...\n",
      "config: 4KB-max-tied-1clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:50<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 4KB-max-tied-16clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:03<00:00, 39.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 4KB-max-tied-64clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:01<00:00, 64.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing file size 256K ...\n",
      "config: 256KB-max-tied-1clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:39<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 256KB-max-tied-16clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:02<00:00, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 256KB-max-tied-64clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:02<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing file size 1024K ...\n",
      "config: 1024KB-max-tied-1clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:39<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 1024KB-max-tied-16clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:03<00:00, 41.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 1024KB-max-tied-64clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:02<00:00, 53.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing file size 4096K ...\n",
      "config: 4096KB-max-tied-1clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:39<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 4096KB-max-tied-16clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:03<00:00, 34.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: 4096KB-max-tied-64clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accessing S3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:03<00:00, 40.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for file_size in file_size_kb:\n",
    "    byte_file = '_test_{0}K.bin'.format(file_size)\n",
    "    bytez = bytearray([0 for i in range(file_size * 1024)])\n",
    "    with open(byte_file, 'wb') as f:\n",
    "        f.write(bytez)\n",
    "        \n",
    "    print('preparing file size {0}K ...'.format(file_size))\n",
    "    for i in range(128):\n",
    "        obj_name = \"testObj%08d\"%(i,) # 所建对象名\n",
    "        s3.Object(bucket_name, obj_name).upload_file(byte_file)\n",
    "        \n",
    "    for limit_name, limit in rate_limits.items():\n",
    "        for policy_name, policy in policies.items():\n",
    "            for n_client in client_count:\n",
    "\n",
    "                print('config: {0}KB-{1}-{2}-{3}clients'.format(file_size, limit_name, policy_name, n_client))\n",
    "                ltcy, failed = bench(byte_file, limit, policy, n_client)\n",
    "\n",
    "                if len(failed) > 0:\n",
    "                    try:\n",
    "                        failed[0].result()\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                with open(\"download-latency-{0}KB-{1}-{2}-{3}clients.csv\".format(file_size, limit_name, policy_name, n_client), \"w\") as tracefile:\n",
    "                    tracefile.writelines([str(l) + '\\n' for l in ltcy])\n",
    "                    \n",
    "    bucket.objects.filter().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f487011d6c19380a0f12fb28ca8659fdd2afd3ccddcfad5755f4bb9819179fc2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
